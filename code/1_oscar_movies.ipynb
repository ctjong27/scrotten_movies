{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.imdb.com/search/name/?gender=male&sort=birth_date,asc&count=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Google colab specific\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# from tqdm import tqdm\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from google.colab import drive\n",
    "\n",
    "# def all_imdb_pages(start_url, test_mode=False, max_pages=10):\n",
    "#     all_urls = []  # list to get all urls\n",
    "#     url = start_url  # begin page\n",
    "#     page_count = 0  # initialize page counter\n",
    "\n",
    "#     # Get total number of names (for non-test mode)\n",
    "#     if not test_mode:\n",
    "#         soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "#         desc = soup.find('div', class_='desc').get_text()\n",
    "#         total_names = int(re.search(r\"of (\\d+)\", desc).group(1))  # Extract total number of names\n",
    "#         max_pages = (total_names // 250) + 1  # Calculate total pages, assuming 250 names per page\n",
    "\n",
    "#         print(f\"total_names: {total_names}\")\n",
    "#         print(f\"max_pages: {max_pages}\")\n",
    "\n",
    "#     for _ in tqdm(range(max_pages), desc=\"Scraping pages\"):  # Loop through all the webpages\n",
    "#         all_urls.append(url)  # add to the list\n",
    "#         soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        \n",
    "#         next_links = soup.find_all(class_='lister-page-next next-page')  # Extracts the next page link\n",
    "#         if len(next_links) == 0:  # If there is no next page, it returns 0\n",
    "#             url = None\n",
    "#         else:\n",
    "#             next_page = \"https://www.imdb.com\" + next_links[0].get('href')\n",
    "#             url = next_page\n",
    "        \n",
    "#         if url is None or page_count == max_pages:\n",
    "#             break\n",
    "#         else:\n",
    "#             page_count += 1  # increment page counter\n",
    "\n",
    "#     return all_urls\n",
    "\n",
    "# def scrape_page(url):\n",
    "#     names = []\n",
    "#     links = []\n",
    "#     ids = []\n",
    "\n",
    "#     soup = BeautifulSoup(requests.get(url).text, 'html.parser')  # Extracts out the main HTML code\n",
    "#     lister_items = soup.find_all('div', class_='lister-item-content')  # Get all the containers\n",
    "\n",
    "#     for item in lister_items:  # loop through all the people in the container to get the attributes\n",
    "#         name_tag = item.find('h3', class_='lister-item-header').find('a')\n",
    "#         name = name_tag.text.strip()\n",
    "#         link = name_tag['href']\n",
    "#         id = link.split('/')[2]\n",
    "#         names.append(name)\n",
    "#         links.append(link)\n",
    "#         ids.append(id)\n",
    "\n",
    "#     return names, links, ids\n",
    "\n",
    "# def get_imdb_info(url, test_mode=False, max_pages=10):\n",
    "#     all_names = []\n",
    "#     all_links = []\n",
    "#     all_ids = []\n",
    "\n",
    "#     page_urls = all_imdb_pages(url, test_mode, max_pages)  # get all page urls\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=20) as executor:  # Change max_workers to a higher number\n",
    "#         for names, links, ids in executor.map(scrape_page, page_urls):\n",
    "#             all_names.extend(names)\n",
    "#             all_links.extend(links)\n",
    "#             all_ids.extend(ids)\n",
    "\n",
    "#     df = pd.DataFrame({\n",
    "#         'Name': all_names,\n",
    "#         'Link': all_links,\n",
    "#         'ID': all_ids\n",
    "#     })\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def all_imdb_pages(start_url, test_mode=False, max_pages=10):\n",
    "    all_urls = []  # list to get all urls\n",
    "    url = start_url  # begin page\n",
    "    page_count = 0  # initialize page counter\n",
    "\n",
    "    # Get total number of names (for non-test mode)\n",
    "    if not test_mode:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        desc = soup.find('div', class_='desc').get_text()\n",
    "        total_names_str = re.search(r\"of ([\\d,]+)\", desc).group(1)  # Extract total number of names\n",
    "        total_names = int(total_names_str.replace(',', ''))  # Remove comma and convert to int\n",
    "        max_pages = (total_names // 250) + 1  # Calculate total pages, assuming 250 names per page\n",
    "        print(f\"total_names: {total_names}\")\n",
    "        print(f\"max_pages: {max_pages}\")\n",
    "\n",
    "    for _ in tqdm(range(max_pages), desc=\"Scraping pages\"):  # Loop through all the webpages\n",
    "        all_urls.append(url)  # add to the list\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        \n",
    "        next_links = soup.find_all(class_='lister-page-next next-page')  # Extracts the next page link\n",
    "        if len(next_links) == 0:  # If there is no next page, it returns 0\n",
    "            url = None\n",
    "        else:\n",
    "            next_page = \"https://www.imdb.com\" + next_links[0].get('href')\n",
    "            url = next_page\n",
    "        \n",
    "        if url is None or page_count == max_pages:\n",
    "            break\n",
    "        else:\n",
    "            page_count += 1  # increment page counter\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "def scrape_page(url):\n",
    "    names = []\n",
    "    links = []\n",
    "    ids = []\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')  # Extracts out the main HTML code\n",
    "    lister_items = soup.find_all('div', class_='lister-item-content')  # Get all the containers\n",
    "\n",
    "    for item in lister_items:  # loop through all the people in the container to get the attributes\n",
    "        name_tag = item.find('h3', class_='lister-item-header').find('a')\n",
    "        name = name_tag.text.strip()\n",
    "        link = name_tag['href']\n",
    "        id = link.split('/')[2]\n",
    "        names.append(name)\n",
    "        links.append(link)\n",
    "        ids.append(id)\n",
    "\n",
    "    return names, links, ids\n",
    "\n",
    "def get_imdb_info(url, test_mode=False, max_pages=10):\n",
    "    all_names = []\n",
    "    all_links = []\n",
    "    all_ids = []\n",
    "\n",
    "    page_urls = all_imdb_pages(url, test_mode, max_pages)  # get all page urls\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:  # Change max_workers to a higher number\n",
    "        for names, links, ids in executor.map(scrape_page, page_urls):\n",
    "            all_names.extend(names)\n",
    "            all_links.extend(links)\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Name': all_names,\n",
    "        'Link': all_links,\n",
    "        'ID': all_ids\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_names: 5073\n",
      "max_pages: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  95%|█████████▌| 20/21 [02:00<00:06,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped and saved data for oscar movies in the 'data' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the gender\n",
    "award_type = 'oscar'\n",
    "\n",
    "# Specify the start URL based on the gender\n",
    "start_url = f'https://www.imdb.com/search/title/?groups={award_type}_nominee&sort=release_date,asc&count=250'\n",
    "\n",
    "# Scrape the IMDb info\n",
    "df = get_imdb_info(start_url, test_mode=False)\n",
    "\n",
    "# Specify the output path for saving the data\n",
    "output_folder = 'data'\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "output_path = os.path.join(output_folder, f'{award_type}_movies.csv')\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Scraped and saved data for {award_type} movies in the '{output_folder}' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
