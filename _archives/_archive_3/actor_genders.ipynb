{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.imdb.com/search/name/?gender=male&sort=birth_date,asc&count=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_names: 4619034\n",
      "max_pages: 18477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 3/18477 [00:04<7:06:55,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m start_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://www.imdb.com/search/name/?gender=\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m&sort=birth_date,asc&count=250\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[39m# Scrape the IMDb info\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m df \u001b[39m=\u001b[39m get_imdb_info(start_url, test_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     93\u001b[0m \u001b[39m# Specify the output file path for saving the data\u001b[39;00m\n\u001b[0;32m     94\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m_actors.csv\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36mget_imdb_info\u001b[1;34m(url, test_mode, max_pages)\u001b[0m\n\u001b[0;32m     62\u001b[0m all_links \u001b[39m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m all_ids \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 65\u001b[0m page_urls \u001b[39m=\u001b[39m all_imdb_pages(url, test_mode, max_pages)  \u001b[39m# get all page urls\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m) \u001b[39mas\u001b[39;00m executor:  \u001b[39m# Change max_workers to a higher number\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mfor\u001b[39;00m names, links, ids \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(scrape_page, page_urls):\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mall_imdb_pages\u001b[1;34m(start_url, test_mode, max_pages)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(max_pages), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScraping pages\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# Loop through all the webpages\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     all_urls\u001b[39m.\u001b[39mappend(url)  \u001b[39m# add to the list\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(requests\u001b[39m.\u001b[39;49mget(url)\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# parser\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     next_links \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlister-page-next next-page\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Extracts the next page link\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(next_links) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# If there is no next page, it returns 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\urllib3\\connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def all_imdb_pages(start_url, test_mode=False, max_pages=10):\n",
    "    all_urls = []  # list to get all urls\n",
    "    url = start_url  # begin page\n",
    "    page_count = 0  # initialize page counter\n",
    "\n",
    "    # Get total number of names (for non-test mode)\n",
    "    if not test_mode:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        desc = soup.find('div', class_='desc').get_text()\n",
    "        total_names_str = re.search(r\"of ([\\d,]+)\", desc).group(1)  # Extract total number of names\n",
    "        total_names = int(total_names_str.replace(',', ''))  # Remove comma and convert to int\n",
    "        max_pages = (total_names // 250) + 1  # Calculate total pages, assuming 250 names per page\n",
    "        print(f\"total_names: {total_names}\")\n",
    "        print(f\"max_pages: {max_pages}\")\n",
    "\n",
    "    for _ in tqdm(range(max_pages), desc=\"Scraping pages\"):  # Loop through all the webpages\n",
    "        all_urls.append(url)  # add to the list\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        \n",
    "        next_links = soup.find_all(class_='lister-page-next next-page')  # Extracts the next page link\n",
    "        if len(next_links) == 0:  # If there is no next page, it returns 0\n",
    "            url = None\n",
    "        else:\n",
    "            next_page = \"https://www.imdb.com\" + next_links[0].get('href')\n",
    "            url = next_page\n",
    "        \n",
    "        if url is None or page_count == max_pages:\n",
    "            break\n",
    "        else:\n",
    "            page_count += 1  # increment page counter\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "def scrape_page(url):\n",
    "    names = []\n",
    "    links = []\n",
    "    ids = []\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')  # Extracts out the main HTML code\n",
    "    lister_items = soup.find_all('div', class_='lister-item-content')  # Get all the containers\n",
    "\n",
    "    for item in lister_items:  # loop through all the people in the container to get the attributes\n",
    "        name_tag = item.find('h3', class_='lister-item-header').find('a')\n",
    "        name = name_tag.text.strip()\n",
    "        link = name_tag['href']\n",
    "        id = link.split('/')[2]\n",
    "        names.append(name)\n",
    "        links.append(link)\n",
    "        ids.append(id)\n",
    "\n",
    "    return names, links, ids\n",
    "\n",
    "def get_imdb_info(url, test_mode=False, max_pages=10):\n",
    "    all_names = []\n",
    "    all_links = []\n",
    "    all_ids = []\n",
    "\n",
    "    page_urls = all_imdb_pages(url, test_mode, max_pages)  # get all page urls\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:  # Change max_workers to a higher number\n",
    "        for names, links, ids in executor.map(scrape_page, page_urls):\n",
    "            all_names.extend(names)\n",
    "            all_links.extend(links)\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Name': all_names,\n",
    "        'Link': all_links,\n",
    "        'ID': all_ids\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Specify the gender\n",
    "gender = 'male'\n",
    "# gender = 'female'\n",
    "# gender = 'non-binary'\n",
    "# gender = 'other'\n",
    "\n",
    "# Specify the start URL based on the gender\n",
    "start_url = f'https://www.imdb.com/search/name/?gender={gender}&sort=birth_date,asc&count=250'\n",
    "\n",
    "# Scrape the IMDb info\n",
    "df = get_imdb_info(start_url, test_mode=False)\n",
    "\n",
    "# Specify the output file path for saving the data\n",
    "output_file = f'{gender}_actors.csv'\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Scraped and saved data for {gender} actors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_names: 4619034\n",
      "max_pages: 18477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 7/18477 [00:28<20:36:28,  4.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m start_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://www.imdb.com/search/name/?gender=\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m&sort=birth_date,asc&count=250\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[39m# Scrape the IMDb info\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m df \u001b[39m=\u001b[39m get_imdb_info(start_url, test_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     93\u001b[0m \u001b[39m# Specify the output file path for saving the data\u001b[39;00m\n\u001b[0;32m     94\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m_actors.csv\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 64\u001b[0m, in \u001b[0;36mget_imdb_info\u001b[1;34m(url, test_mode, max_pages)\u001b[0m\n\u001b[0;32m     61\u001b[0m all_links \u001b[39m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m all_ids \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 64\u001b[0m page_urls \u001b[39m=\u001b[39m all_imdb_pages(url, test_mode, max_pages)  \u001b[39m# get all page urls\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m) \u001b[39mas\u001b[39;00m executor:  \u001b[39m# Change max_workers to a higher number\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tqdm(executor\u001b[39m.\u001b[39mmap(scrape_page, page_urls), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(page_urls), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScraping actors\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mall_imdb_pages\u001b[1;34m(start_url, test_mode, max_pages)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(max_pages), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScraping pages\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# Loop through all the webpages\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     all_urls\u001b[39m.\u001b[39mappend(url)  \u001b[39m# add to the list\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(requests\u001b[39m.\u001b[39;49mget(url)\u001b[39m.\u001b[39;49mtext, \u001b[39m\"\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# parser\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     next_links \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlister-page-next next-page\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Extracts the next page link\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(next_links) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# If there is no next page, it returns 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\bs4\\__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    336\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\bs4\\__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 478\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    479\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    378\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    381\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    382\u001b[0m     \u001b[39m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[39m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[39m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\html\\parser.py:165\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:j])\n\u001b[1;32m--> 165\u001b[0m i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdatepos(i, j)\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m n: \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    167\u001b[0m startswith \u001b[39m=\u001b[39m rawdata\u001b[39m.\u001b[39mstartswith\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\_markupbase.py:44\u001b[0m, in \u001b[0;36mParserBase.updatepos\u001b[1;34m(self, i, j)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineno, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset\n\u001b[0;32m     40\u001b[0m \u001b[39m# Internal -- update line number and offset.  This should be\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# called for each piece of data exactly once, in order -- in other\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# words the concatenation of all the input strings to this\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39m# function should be exactly the entire input.\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdatepos\u001b[39m(\u001b[39mself\u001b[39m, i, j):\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m j:\n\u001b[0;32m     46\u001b[0m         \u001b[39mreturn\u001b[39;00m j\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def all_imdb_pages(start_url, test_mode=False, max_pages=10):\n",
    "    all_urls = []  # list to get all urls\n",
    "    url = start_url  # begin page\n",
    "    page_count = 0  # initialize page counter\n",
    "\n",
    "    # Get total number of names (for non-test mode)\n",
    "    if not test_mode:\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        desc = soup.find('div', class_='desc').get_text()\n",
    "        total_names_str = re.search(r\"of ([\\d,]+)\", desc).group(1)  # Extract total number of names\n",
    "        total_names = int(total_names_str.replace(',', ''))  # Remove comma and convert to int\n",
    "        max_pages = (total_names // 250) + 1  # Calculate total pages, assuming 250 names per page\n",
    "        print(f\"total_names: {total_names}\")\n",
    "        print(f\"max_pages: {max_pages}\")\n",
    "\n",
    "    for _ in tqdm(range(max_pages), desc=\"Scraping pages\"):  # Loop through all the webpages\n",
    "        all_urls.append(url)  # add to the list\n",
    "        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")  # parser\n",
    "        \n",
    "        next_links = soup.find_all(class_='lister-page-next next-page')  # Extracts the next page link\n",
    "        if len(next_links) == 0:  # If there is no next page, it returns 0\n",
    "            url = None\n",
    "        else:\n",
    "            next_page = \"https://www.imdb.com\" + next_links[0].get('href')\n",
    "            url = next_page\n",
    "        \n",
    "        if url is None or page_count == max_pages:\n",
    "            break\n",
    "        else:\n",
    "            page_count += 1  # increment page counter\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "def scrape_page(url):\n",
    "    names = []\n",
    "    links = []\n",
    "    ids = []\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')  # Extracts out the main HTML code\n",
    "    lister_items = soup.find_all('div', class_='lister-item-content')  # Get all the containers\n",
    "\n",
    "    for item in lister_items:  # loop through all the people in the container to get the attributes\n",
    "        name_tag = item.find('h3', class_='lister-item-header').find('a')\n",
    "        name = name_tag.text.strip()\n",
    "        link = name_tag['href']\n",
    "        id = link.split('/')[2]\n",
    "        names.append(name)\n",
    "        links.append(link)\n",
    "        ids.append(id)\n",
    "\n",
    "    return names, links, ids\n",
    "\n",
    "def get_imdb_info(url, test_mode=False, max_pages=10):\n",
    "    all_names = []\n",
    "    all_links = []\n",
    "    all_ids = []\n",
    "\n",
    "    page_urls = all_imdb_pages(url, test_mode, max_pages)  # get all page urls\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor:  # Change max_workers to a higher number\n",
    "        results = list(tqdm(executor.map(scrape_page, page_urls), total=len(page_urls), desc=\"Scraping actors\"))\n",
    "        for names, links, ids in results:\n",
    "            all_names.extend(names)\n",
    "            all_links.extend(links)\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Name': all_names,\n",
    "        'Link': all_links,\n",
    "        'ID': all_ids\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Specify the gender\n",
    "gender = 'male'\n",
    "# gender = 'female'\n",
    "# gender = 'non-binary'\n",
    "# gender = 'other'\n",
    "\n",
    "# Specify the start URL based on the gender\n",
    "start_url = f'https://www.imdb.com/search/name/?gender={gender}&sort=birth_date,asc&count=250'\n",
    "\n",
    "# Scrape the IMDb info\n",
    "df = get_imdb_info(start_url, test_mode=False)\n",
    "\n",
    "# Specify the output file path for saving the data\n",
    "output_file = f'{gender}_actors.csv'\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Scraped and saved data for {gender} actors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m start_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://www.imdb.com/search/name/?gender=\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m&sort=birth_date,asc&count=250\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[39m# Run the function and get the IMDb info asynchronously\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m df \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(get_imdb_info(start_url, max_pages\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m))\n\u001b[0;32m     66\u001b[0m \u001b[39m# Specify the output file path for saving the data\u001b[39;00m\n\u001b[0;32m     67\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m_actors.csv\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[39mwith\u001b[39;00m Runner(debug\u001b[39m=\u001b[39mdebug) \u001b[39mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m runner\u001b[39m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "async def scrape_page(url):\n",
    "    names = []\n",
    "    links = []\n",
    "    ids = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            soup = BeautifulSoup(await response.text(), 'html.parser')\n",
    "            lister_items = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "            for item in lister_items:\n",
    "                name_tag = item.find('h3', class_='lister-item-header').find('a')\n",
    "                name = name_tag.text.strip()\n",
    "                link = name_tag['href']\n",
    "                id = link.split('/')[2]\n",
    "                names.append(name)\n",
    "                links.append(link)\n",
    "                ids.append(id)\n",
    "\n",
    "    return names, links, ids\n",
    "\n",
    "async def get_imdb_info(url, max_pages=10):\n",
    "    all_names = []\n",
    "    all_links = []\n",
    "    all_ids = []\n",
    "\n",
    "    tasks = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        page_url = f\"{url}&page={page}\"\n",
    "        task = asyncio.ensure_future(scrape_page(page_url))\n",
    "        tasks.append(task)\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for names, links, ids in results:\n",
    "        all_names.extend(names)\n",
    "        all_links.extend(links)\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Name': all_names,\n",
    "        'Link': all_links,\n",
    "        'ID': all_ids\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Specify the gender\n",
    "gender = 'male'\n",
    "# gender = 'female'\n",
    "# gender = 'non-binary'\n",
    "# gender = 'other'\n",
    "\n",
    "# Specify the start URL based on the gender\n",
    "start_url = f'https://www.imdb.com/search/name/?gender={gender}&sort=birth_date,asc&count=250'\n",
    "\n",
    "# Run the function and get the IMDb info asynchronously\n",
    "df = asyncio.run(get_imdb_info(start_url, max_pages=10))\n",
    "\n",
    "# Specify the output file path for saving the data\n",
    "output_file = f'{gender}_actors.csv'\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Scraped and saved data for {gender} actors\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_names: 2513828\n",
      "max_pages: 10056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   0%|          | 5/10056 [01:19<44:13:36, 15.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m start_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://www.imdb.com/search/name/?gender=\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m&sort=birth_date,asc&count=250\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Scrape the IMDb info\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df \u001b[39m=\u001b[39m get_imdb_info(start_url, test_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Specify the output file path for saving the data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mgender\u001b[39m}\u001b[39;00m\u001b[39m_actors.csv\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36mget_imdb_info\u001b[1;34m(url, test_mode, max_pages)\u001b[0m\n\u001b[0;32m     62\u001b[0m all_links \u001b[39m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m all_ids \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 65\u001b[0m page_urls \u001b[39m=\u001b[39m all_imdb_pages(url, test_mode, max_pages)  \u001b[39m# get all page urls\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m) \u001b[39mas\u001b[39;00m executor:  \u001b[39m# Change max_workers to a higher number\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mfor\u001b[39;00m names, links, ids \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(scrape_page, page_urls):\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mall_imdb_pages\u001b[1;34m(start_url, test_mode, max_pages)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(max_pages), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mScraping pages\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# Loop through all the webpages\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     all_urls\u001b[39m.\u001b[39mappend(url)  \u001b[39m# add to the list\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(requests\u001b[39m.\u001b[39;49mget(url)\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# parser\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     next_links \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlister-page-next next-page\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Extracts the next page link\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(next_links) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# If there is no next page, it returns 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\cjong\\Projects\\python\\scrotten_movies\\.venv\\Lib\\site-packages\\urllib3\\connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the gender\n",
    "# gender = 'male'\n",
    "gender = 'female'\n",
    "# gender = 'non-binary'\n",
    "# gender = 'other'\n",
    "\n",
    "# Specify the start URL based on the gender\n",
    "start_url = f'https://www.imdb.com/search/name/?gender={gender}&sort=birth_date,asc&count=250'\n",
    "\n",
    "# Scrape the IMDb info\n",
    "df = get_imdb_info(start_url, test_mode=False)\n",
    "\n",
    "# Specify the output file path for saving the data\n",
    "output_file = f'{gender}_actors.csv'\n",
    "\n",
    "# Save the data to a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Scraped and saved data for {gender} actors\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
